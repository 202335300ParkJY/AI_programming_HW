{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gRdC8rlvLZj"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_grad_graph(grad_fn, level=0):\n",
        "    print(' ' * level, grad_fn)\n",
        "    if hasattr(grad_fn, 'next_functions'):\n",
        "        for next_func in grad_fn.next_functions:\n",
        "            if next_func[0] is not None:\n",
        "                print_grad_graph(next_func[0], level + 1)\n",
        "\n",
        "# 사용 예시\n",
        "import torch\n",
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "y = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
        "z = x * y\n",
        "w = z.sum()\n",
        "\n",
        "print_grad_graph(w.grad_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dotTUDfxGf7",
        "outputId": "62189af1-2b5a-441a-cd64-6634347d32e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <SumBackward0 object at 0x7bb307d1bdf0>\n",
            "  <MulBackward0 object at 0x7bb307d1b6a0>\n",
            "   <AccumulateGrad object at 0x7bb307d1ba30>\n",
            "   <AccumulateGrad object at 0x7bb307d1bca0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_grad_graph(grad_fn, level=0):\n",
        "    print(' ' * level, type(grad_fn).__name__,grad_fn)\n",
        "    if hasattr(grad_fn, 'next_functions'):\n",
        "      for next_func in grad_fn.next_functions:\n",
        "        if next_func[0] is not None:\n",
        "          print_grad_graph(next_func[0], level + 1)\n",
        "\n",
        "w = z.sum()\n",
        "print_grad_graph(w.grad_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DDQl3YcyBEQ",
        "outputId": "82181345-05cf-446d-ecd6-129e7dcf0336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " SumBackward0 <SumBackward0 object at 0x7bb308b384f0>\n",
            "  MulBackward0 <MulBackward0 object at 0x7bb308b396f0>\n",
            "   AccumulateGrad <AccumulateGrad object at 0x7bb308b3b910>\n",
            "   AccumulateGrad <AccumulateGrad object at 0x7bb308b397e0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Custom Autograd Func 만들기\n",
        "실제론 ctx(컨텍스트)에 저장됨. (중요!)\n",
        "- 모든 diff. op.는 torch.autograd.Func. class의 상속.\n",
        "- Custom autograd func. 정의 시 필수 구현 (static) 메서드는 forward(): forward computation 수행. / backward(): gradient computation 구현.\n"
      ],
      "metadata": {
        "id": "mSdH3zxtzAvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Function\n",
        "\n",
        "class CustomFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(input)\n",
        "        output = input.clone()\n",
        "        # 원하는 op 수행\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        # 원하는 gradient 계산\n",
        "        return grad_input"
      ],
      "metadata": {
        "id": "9awIP1kOy88z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2-1. custom func. 사용법\n",
        "\n",
        "# 2-1-1. apply 정적 method로 호출"
      ],
      "metadata": {
        "id": "SwCU5eIazqzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCustomFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, t):\n",
        "      ctx.save_for_backward(x, y)\n",
        "      return x * y\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(cts, grad_output):\n",
        "      x, y = ctx.saved_tensors\n",
        "      return grad_output * y, grad_output * x\n",
        "\n",
        "custom_op = MyCustomFunction.apply\n",
        "a = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(2.0, requires_grad=True)\n",
        "c = custom_op(a, b)\n",
        "\n",
        "print(\"c.grad_fn:\", c.grad_fn)\n",
        "print(\"c.grad_fn.next_functions:\", c.grad_fn.next_functions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbyz5BG3zeWx",
        "outputId": "6928caa4-6438-480f-afda-6de9ea688c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c.grad_fn: <torch.autograd.function.MyCustomFunctionBackward object at 0x7bb3089d5bf0>\n",
            "c.grad_fn.next_functions: ((<AccumulateGrad object at 0x7bb307ff7940>, 0), (<AccumulateGrad object at 0x7bb307ff4af0>, 0))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "apply 메서드 역할:\n",
        "\n",
        "forward method 호출과 순방향 계산 수행.\n",
        "계산 graph 구성 및 backward method와의 연결.\n",
        "결과 tensor의 반환과 grad_fn 설정.\n"
      ],
      "metadata": {
        "id": "wQmtj0V8JnVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2-1-2. Cllable 객체를 통한 직관적 방법"
      ],
      "metadata": {
        "id": "kqrRgReq0h-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Func. class를 직접 호출 가능한 형태로 래핑하는 방법:\n",
        "# 방법 1: callable class 활용\n",
        "class CustomOp:\n",
        "    def __call__(self, x, y):\n",
        "        return MyCustomFunction.apply(x, y)\n",
        "\n",
        "# 인스턴스 생성\n",
        "custom_op = CustomOp()\n",
        "result = custom_op(a, b)  # 일반 함수처럼 호출\n",
        "# 방법 2: 함수 래퍼 활용 (보다 간결한 접근)\n",
        "def custom_op_func(x, y):\n",
        "    return MyCustomFunction.apply(x, y)\n",
        "\n",
        "result = custom_op_func(a, b)  # 일반 함수처럼 호출"
      ],
      "metadata": {
        "id": "rYrftlQ80YE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Callable 래퍼 사용의 장점:\n",
        "\n",
        ".apply method 직접 호출 불필요.\n",
        "더 직관적 함수 호출 구문 제공.\n",
        "함수형 프로그래밍 패턴과의 일관성."
      ],
      "metadata": {
        "id": "4QJfB8s5KV7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.next_func. 자동 상속 메커니즘\n",
        "torch.autograd.Func. 상속 시 next_functions attribute의 자동 포함.\n",
        "User가 명시적으로 next_functions 구현 불필요.\n",
        "\n",
        "PyTorch의 내부 process는\n",
        "\n",
        "*   Operation 결과에 해당 operation type의 grad_fn 연결.\n",
        "*   grad_fn의 next_functions에 input tensor들의 grad_fn 자동 저장.\n"
      ],
      "metadata": {
        "id": "tzhuQpT108ej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3-1.next_func. attri. 구조\n",
        "\n",
        "\n",
        "\n",
        "*   Tuple of tuples 형태의 type임\n",
        "*   각 tuple의 첫 번째 요소는 input tensor의 grad_fn, 두 번째 요소는 input index.\n",
        "*   Input tensor들이 forward 함수에 전달된 순서와 동일한 순서로 저장됨.\n",
        "*   requires_grad=False인 tensor의 경우 (None, 0) 형태로 저장.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UujJWfNl1BlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3-2.다중 output인 op.예시"
      ],
      "metadata": {
        "id": "brakJ2Wy1I4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 하나의 텐서를 두 부분으로 분할\n",
        "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
        "a, b = torch.chunk(x, 2)  # 두 개의 결과물 생성\n",
        "c = a.sum() + b.sum()\n",
        "\n",
        "print(\"c.grad_fn:\", c.grad_fn)\n",
        "print(\"c.grad_fn.next_functions:\", c.grad_fn.next_functions)\n",
        "\n",
        "# 각 SumBackward0의 next_functions\n",
        "for i, (grad_fn, idx) in enumerate(c.grad_fn.next_functions):\n",
        "    print(f\"grad_fn[{i}].next_functions:\", grad_fn.next_functions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROqLCRzC1Rnm",
        "outputId": "fa3d413e-5791-43a8-a08d-de30d5f0b130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c.grad_fn: <AddBackward0 object at 0x7bb307ef3fd0>\n",
            "c.grad_fn.next_functions: ((<SumBackward0 object at 0x7bb307ef20e0>, 0), (<SumBackward0 object at 0x7bb307ef0070>, 0))\n",
            "grad_fn[0].next_functions: ((<SplitBackward0 object at 0x7bb307ef30d0>, 0),)\n",
            "grad_fn[1].next_functions: ((<SplitBackward0 object at 0x7bb307ef30d0>, 1),)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   torch.chunk는 하나의 tensor를 여러 부분으로 나누는 op, torch.chunk(tensor, chunks, dim=0): 지정된 dimension을 따라 tensor를 chunks 개수만큼 균등하게 분할.\n",
        "*   각 chunk는 원본 tensor의 크기를 chunks로 나눈 크기를 가짐.\n",
        "*   dim=0을 기준으로 분할하나, dim parameter를 통해 다른 dimension 기준 분할 가능.\n",
        "*   각 chunk에 대한 gradient 계산 시 ChunkBackward의 서로 다른 output(index 0과 1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-B0eBzqfLxAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCustomFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, t):\n",
        "      ctx.save_for_backward(x, y)\n",
        "      return x * y\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "      x, y = ctx.saved_tensors\n",
        "      return grad_output * y, grad_output * x\n",
        "\n",
        "custom_op = MyCustomFunction.apply\n",
        "a = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(2.0, requires_grad=True)\n",
        "c = custom_op(a, b)\n",
        "\n",
        "print(\"c.grad_fn:\", c.grad_fn)\n",
        "print(\"c.grad_fn.next_functions:\", c.grad_fn.next_functions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQm2WUQY1xYH",
        "outputId": "e2d7093f-f831-4e3f-beec-6f4cbfff6a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c.grad_fn: <torch.autograd.function.MyCustomFunctionBackward object at 0x7bb402b9dbf0>\n",
            "c.grad_fn.next_functions: ((<AccumulateGrad object at 0x7bb307d552a0>, 0), (<AccumulateGrad object at 0x7bb307d55e40>, 0))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   backpropagation 시 전체 computation graph의 추적 가능.\n",
        "*   Autograd system의 효율적인 gradient computation의 기반.\n",
        "\n"
      ],
      "metadata": {
        "id": "zTqMM634Pab2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://dabletech.oopy.io/99645b3b-d58a-4225-9253-3589b98c5bb1\n",
        "라는 링크 안에 pytorch autograd(오태호 작성자)\n",
        "내용 추천해주심\n",
        "\n",
        "15,17 정도는 해보면 좋겟지만, 20까지 하면 직접 만들 수 있음!"
      ],
      "metadata": {
        "id": "dz80u7Rc22n8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o20RlCD_2cLg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}