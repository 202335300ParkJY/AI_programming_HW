{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LmOLtKrfWIIb"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code1"
      ],
      "metadata": {
        "id": "M-D10OE7XVOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(5.0)\n",
        "y = x ** 3\n",
        "z = torch.log(y)\n",
        "\n",
        "print('x', x)\n",
        "print('y', y)\n",
        "print('z', z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_uClz7kW-eh",
        "outputId": "69169132-ccb8-43c6-e124-b93addd7eb2c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x tensor(5.)\n",
            "y tensor(125.)\n",
            "z tensor(4.8283)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 2"
      ],
      "metadata": {
        "id": "zwGDD7LxXXT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(5.0)\n",
        "y = x ** 3\n",
        "z = torch.log(y)\n",
        "\n",
        "x2 = torch.tensor(5.001)\n",
        "y2 = x2 ** 3\n",
        "z2 = torch.log(y2)\n",
        "\n",
        "print('x', x)\n",
        "print('y', y)\n",
        "print('z', z)\n",
        "\n",
        "print('x2', x2)\n",
        "print('y2', y2)\n",
        "print('z2', z2)\n",
        "\n",
        "print('dz/dx', (z2 - z) / (x2 - x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eORp92B2XTOm",
        "outputId": "75a034cd-264c-4b21-a7ee-26cd846f9142"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x tensor(5.)\n",
            "y tensor(125.)\n",
            "z tensor(4.8283)\n",
            "x2 tensor(5.0010)\n",
            "y2 tensor(125.0750)\n",
            "z2 tensor(4.8289)\n",
            "dz/dx tensor(0.5999)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 3"
      ],
      "metadata": {
        "id": "pXoythA3X7US"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_info(tensor):\n",
        "  info = []\n",
        "  for name in ['requires_grad', 'is_leaf', 'grad_fn', 'grad']:\n",
        "    info.append(f'{name}({getattr(tensor, name)})')\n",
        "  info.append(f'tensor({str(tensor)})')\n",
        "  return ' '.join(info)\n",
        "\n",
        "x = torch.tensor(5.0)\n",
        "y = x ** 3\n",
        "z = torch.log(y)\n",
        "\n",
        "print('x', get_tensor_info(x))\n",
        "print('y', get_tensor_info(y))\n",
        "print('z', get_tensor_info(z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX_2_g9lXoJj",
        "outputId": "e4cc3f2c-9c55-4b72-a32f-6a0b42ee6356"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x requires_grad(False) is_leaf(True) grad_fn(None) grad(None) tensor(tensor(5.))\n",
            "y requires_grad(False) is_leaf(True) grad_fn(None) grad(None) tensor(tensor(125.))\n",
            "z requires_grad(False) is_leaf(True) grad_fn(None) grad(None) tensor(tensor(4.8283))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 4"
      ],
      "metadata": {
        "id": "NjVvgwAvZIS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_info(tensor):\n",
        "  info = []\n",
        "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
        "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
        "  info.append(f'tensor({str(tensor)})')\n",
        "  return ' '.join(info)\n",
        "\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "y = x ** 3\n",
        "z = torch.log(y)\n",
        "\n",
        "print('x', get_tensor_info(x))\n",
        "print('y', get_tensor_info(y))\n",
        "print('z', get_tensor_info(z))\n",
        "\n",
        "z.backward()\n",
        "\n",
        "print('x_after_backward', get_tensor_info(x))\n",
        "print('y_after_backward', get_tensor_info(y))\n",
        "print('z_after_backward', get_tensor_info(z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7944e36Y7cH",
        "outputId": "5412e833-9ffa-4331-d450-d56beb88d154"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
            "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b0149552620>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b0149552e90>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
            "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
            "y_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b0149552e90>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "z_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b0149552620>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-53a4dca47c00>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 5"
      ],
      "metadata": {
        "id": "6NXLiXJKaNKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_info(tensor):\n",
        "  info = []\n",
        "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
        "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
        "  info.append(f'tensor({str(tensor)})')\n",
        "  return ' '.join(info)\n",
        "\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "y = x ** 3\n",
        "z = torch.log(y)\n",
        "\n",
        "print('x', get_tensor_info(x))\n",
        "print('y', get_tensor_info(y))\n",
        "print('z', get_tensor_info(z))\n",
        "\n",
        "y.retain_grad()\n",
        "z.retain_grad()\n",
        "z.backward()\n",
        "\n",
        "print('x_after_backward', get_tensor_info(x))\n",
        "print('y_after_backward', get_tensor_info(y))\n",
        "print('z_after_backward', get_tensor_info(z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu_QuB_PZkQW",
        "outputId": "8da019f3-ed1c-4750-b4ed-aaebee61fb0a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
            "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b0149550880>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b01495527a0>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
            "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
            "y_after_backward requires_grad(True) is_leaf(False) retains_grad(True) grad_fn(<PowBackward0 object at 0x7b01495527a0>) grad(0.00800000037997961) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "z_after_backward requires_grad(True) is_leaf(False) retains_grad(True) grad_fn(<LogBackward0 object at 0x7b0149550880>) grad(1.0) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-e182a4bcc5f4>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 6"
      ],
      "metadata": {
        "id": "h6iu7InYaah5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_info(tensor):\n",
        "  info = []\n",
        "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
        "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
        "  info.append(f'tensor({str(tensor)})')\n",
        "  return ' '.join(info)\n",
        "\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "y = x ** 3\n",
        "z = torch.log(y)\n",
        "\n",
        "print('x', get_tensor_info(x))\n",
        "print('y', get_tensor_info(y))\n",
        "print('z', get_tensor_info(z))\n",
        "\n",
        "z.backward()\n",
        "\n",
        "print('x_after_backward', get_tensor_info(x))\n",
        "print('y_after_backward', get_tensor_info(y))\n",
        "print('z_after_backward', get_tensor_info(z))\n",
        "\n",
        "z.backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "UA9LND2HaQKJ",
        "outputId": "021d54fb-03d4-47d4-b0cf-ebd432a1ea96"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
            "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b0149551180>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b01495534f0>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
            "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
            "y_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b01495534f0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "z_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b0149551180>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-ffa6d44d0121>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-ffa6d44d0121>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'z_after_backward'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_tensor_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 7"
      ],
      "metadata": {
        "id": "r58IQToYfXAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_info(tensor):\n",
        "  info = []\n",
        "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
        "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
        "  info.append(f'tensor({str(tensor)})')\n",
        "  return ' '.join(info)\n",
        "\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "y = x ** 3\n",
        "z = torch.log(y)\n",
        "\n",
        "print('x', get_tensor_info(x))\n",
        "print('y', get_tensor_info(y))\n",
        "print('z', get_tensor_info(z))\n",
        "\n",
        "z.backward(retain_graph=True)\n",
        "\n",
        "print('x_after_backward', get_tensor_info(x))\n",
        "print('y_after_backward', get_tensor_info(y))\n",
        "print('z_after_backward', get_tensor_info(z))\n",
        "\n",
        "z.backward()\n",
        "\n",
        "print('x_after_2backward', get_tensor_info(x))\n",
        "print('y_after_2backward', get_tensor_info(y))\n",
        "print('z_after_2backward', get_tensor_info(z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xj6UYEuEad-w",
        "outputId": "c89dee01-bb20-441e-9b48-6d3e5c4583cd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
            "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b013977aef0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b013977bb80>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
            "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
            "y_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b013977bb80>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "z_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b013977aef0>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
            "x_after_2backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(1.2000000476837158) tensor(tensor(5., requires_grad=True))\n",
            "y_after_2backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b013977aef0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "z_after_2backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b013977bb80>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-f05b06e5dd0b>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 8"
      ],
      "metadata": {
        "id": "0jjtuEeRgkKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_info(tensor):\n",
        "  info = []\n",
        "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
        "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
        "  info.append(f'tensor({str(tensor)})')\n",
        "  return ' '.join(info)\n",
        "\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "y = x ** 3\n",
        "z = torch.log(y)\n",
        "\n",
        "print('x', get_tensor_info(x))\n",
        "print('y', get_tensor_info(y))\n",
        "print('z', get_tensor_info(z))\n",
        "\n",
        "z.backward(retain_graph=True)\n",
        "\n",
        "print('x_after_backward', get_tensor_info(x))\n",
        "print('y_after_backward', get_tensor_info(y))\n",
        "print('z_after_backward', get_tensor_info(z))\n",
        "\n",
        "x.grad.zero_()\n",
        "z.backward()\n",
        "\n",
        "print('x_after_2backward', get_tensor_info(x))\n",
        "print('y_after_2backward', get_tensor_info(y))\n",
        "print('z_after_2backward', get_tensor_info(z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0657tHGWgXKP",
        "outputId": "7bee3bb2-1644-43e6-ebc3-ac8f2d3681d5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
            "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b0139c19ab0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b0139c1a710>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
            "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
            "y_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b0139c1a710>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "z_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b0139c19ab0>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
            "x_after_2backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
            "y_after_2backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b0139c19ab0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "z_after_2backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b0139c1a710>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-22d4de4822ad>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 9"
      ],
      "metadata": {
        "id": "-LpXEZxNhUmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_info(tensor):\n",
        "  info = []\n",
        "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
        "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
        "  info.append(f'tensor({str(tensor)})')\n",
        "  return ' '.join(info)\n",
        "\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "y = x ** 3\n",
        "w = x ** 2\n",
        "z = torch.log(y) + torch.sqrt(w)\n",
        "\n",
        "print('x', get_tensor_info(x))\n",
        "print('y', get_tensor_info(y))\n",
        "print('w', get_tensor_info(w))\n",
        "print('z', get_tensor_info(z))\n",
        "\n",
        "z.backward()\n",
        "\n",
        "print('x_after_backward', get_tensor_info(x))\n",
        "print('y_after_backward', get_tensor_info(y))\n",
        "print('w_after_backward', get_tensor_info(w))\n",
        "print('z_after_backward', get_tensor_info(z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsYK1yZOhOFB",
        "outputId": "910b3ee5-f46c-4bd9-9fba-70956b2c0939"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
            "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b0139c18640>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "w requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b0139c18490>) grad(None) tensor(tensor(25., grad_fn=<PowBackward0>))\n",
            "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<AddBackward0 object at 0x7b0139c1ba60>) grad(None) tensor(tensor(9.8283, grad_fn=<AddBackward0>))\n",
            "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(1.600000023841858) tensor(tensor(5., requires_grad=True))\n",
            "y_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b0139c1ba60>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "w_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b0139c18490>) grad(None) tensor(tensor(25., grad_fn=<PowBackward0>))\n",
            "z_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<AddBackward0 object at 0x7b0139c1ba60>) grad(None) tensor(tensor(9.8283, grad_fn=<AddBackward0>))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-33dc9e7d9adb>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 10"
      ],
      "metadata": {
        "id": "vmWiGxdsiZ2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_info(tensor):\n",
        "  info = []\n",
        "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
        "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
        "  info.append(f'tensor({str(tensor)})')\n",
        "  return ' '.join(info)\n",
        "\n",
        "q = torch.tensor(3.0, requires_grad=True)\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "y = x ** q\n",
        "z = torch.log(y)\n",
        "\n",
        "print('q', get_tensor_info(q))\n",
        "print('x', get_tensor_info(x))\n",
        "print('y', get_tensor_info(y))\n",
        "print('z', get_tensor_info(z))\n",
        "\n",
        "z.backward()\n",
        "\n",
        "print('q_after_backward', get_tensor_info(q))\n",
        "print('x_after_backward', get_tensor_info(x))\n",
        "print('y_after_backward', get_tensor_info(y))\n",
        "print('z_after_backward', get_tensor_info(z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1fNMK8BhZ7G",
        "outputId": "fbb3bd7d-ad86-4654-8bc0-36912de78932"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(3., requires_grad=True))\n",
            "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
            "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward1 object at 0x7b0139c1a3b0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward1>))\n",
            "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b0139c1a710>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
            "q_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(1.6094380617141724) tensor(tensor(3., requires_grad=True))\n",
            "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
            "y_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward1 object at 0x7b0139c1a3b0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward1>))\n",
            "z_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b0139c1a710>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-6d2f315e17d5>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 11"
      ],
      "metadata": {
        "id": "EZzDp45Hijnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_info(tensor):\n",
        "  info = []\n",
        "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
        "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
        "  info.append(f'tensor({str(tensor)})')\n",
        "  return ' '.join(info)\n",
        "\n",
        "class MyPow(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, input_1, input_2):\n",
        "    ctx.save_for_backward(input_1, input_2)\n",
        "    result = input_1 ** input_2\n",
        "    return result\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    input_1, input_2 = ctx.saved_tensors\n",
        "    grad_input_1 = grad_output * input_2 * input_1 ** (input_2 - 1)\n",
        "    grad_input_2 = grad_output * input_1 ** input_2 * torch.log(input_1)\n",
        "    print('input_1', input_1)\n",
        "    print('input_2', input_2)\n",
        "    print('grad_output', grad_output)\n",
        "    print('grad_input_1', grad_input_1)\n",
        "    print('grad_input_2', grad_input_2)\n",
        "    return grad_input_1, grad_input_2\n",
        "\n",
        "myPow = MyPow.apply\n",
        "\n",
        "q = torch.tensor(3.0, requires_grad=True)\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "y = myPow(x, q)\n",
        "z = torch.log(y)\n",
        "\n",
        "print('q', get_tensor_info(q))\n",
        "print('x', get_tensor_info(x))\n",
        "print('y', get_tensor_info(y))\n",
        "print('z', get_tensor_info(z))\n",
        "\n",
        "z.backward()\n",
        "\n",
        "print('q_after_backward', get_tensor_info(q))\n",
        "print('x_after_backward', get_tensor_info(x))\n",
        "print('y_after_backward', get_tensor_info(y))\n",
        "print('z_after_backward', get_tensor_info(z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHkI3WNGih1M",
        "outputId": "b34273cb-f5fa-4508-c634-31e84209b2e3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(3., requires_grad=True))\n",
            "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
            "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<torch.autograd.function.MyPowBackward object at 0x7b0231129150>) grad(None) tensor(tensor(125., grad_fn=<MyPowBackward>))\n",
            "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b0139747b50>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
            "input_1 tensor(5., requires_grad=True)\n",
            "input_2 tensor(3., requires_grad=True)\n",
            "grad_output tensor(0.0080)\n",
            "grad_input_1 tensor(0.6000)\n",
            "grad_input_2 tensor(1.6094)\n",
            "q_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(1.6094379425048828) tensor(tensor(3., requires_grad=True))\n",
            "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
            "y_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<torch.autograd.function.MyPowBackward object at 0x7b0231129150>) grad(None) tensor(tensor(125., grad_fn=<MyPowBackward>))\n",
            "z_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b0139747f10>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-dc9e395b812c>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 12"
      ],
      "metadata": {
        "id": "7u6TshTIi5eh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_info(tensor):\n",
        "  info = []\n",
        "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
        "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
        "  info.append(f'tensor({str(tensor)})')\n",
        "  return ' '.join(info)\n",
        "\n",
        "with torch.no_grad():\n",
        "  q = torch.tensor(3.0, requires_grad=True)\n",
        "  x = torch.tensor(5.0, requires_grad=True)\n",
        "  y = x ** q\n",
        "  z = torch.log(y)\n",
        "\n",
        "print('q', get_tensor_info(q))\n",
        "print('x', get_tensor_info(x))\n",
        "print('y', get_tensor_info(y))\n",
        "print('z', get_tensor_info(z))\n",
        "\n",
        "z.backward()\n",
        "\n",
        "print('q_after_backward', get_tensor_info(q))\n",
        "print('x_after_backward', get_tensor_info(x))\n",
        "print('y_after_backward', get_tensor_info(y))\n",
        "print('z_after_backward', get_tensor_info(z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "tgU8184Ji3Uq",
        "outputId": "e43277fe-1e6f-4678-ed07-481d951ad734"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(3., requires_grad=True))\n",
            "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
            "y requires_grad(False) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(125.))\n",
            "z requires_grad(False) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(4.8283))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-f527f3f0ff7e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_tensor_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'q_after_backward'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_tensor_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 13"
      ],
      "metadata": {
        "id": "XOuTNlO4i9uF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_info(tensor):\n",
        "  info = []\n",
        "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
        "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
        "  info.append(f'tensor({str(tensor)})')\n",
        "  return ' '.join(info)\n",
        "\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "y = x * torch.tensor([2.0, 3.0, 5.0])\n",
        "z = y @ torch.tensor([4.0, 7.0, 9.0])\n",
        "\n",
        "print('x', get_tensor_info(x))\n",
        "print('y', get_tensor_info(y))\n",
        "print('z', get_tensor_info(z))\n",
        "\n",
        "y.retain_grad()\n",
        "z.retain_grad()\n",
        "z.backward()\n",
        "\n",
        "print('x_after_backward', get_tensor_info(x))\n",
        "print('y_after_backward', get_tensor_info(y))\n",
        "print('z_after_backward', get_tensor_info(z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_5Jxh2Qi8Mc",
        "outputId": "d94189d0-076a-4253-9ebb-8c8981bb9ac3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
            "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<MulBackward0 object at 0x7b0139744b20>) grad(None) tensor(tensor([10., 15., 25.], grad_fn=<MulBackward0>))\n",
            "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<DotBackward0 object at 0x7b0139747820>) grad(None) tensor(tensor(370., grad_fn=<DotBackward0>))\n",
            "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(74.0) tensor(tensor(5., requires_grad=True))\n",
            "y_after_backward requires_grad(True) is_leaf(False) retains_grad(True) grad_fn(<MulBackward0 object at 0x7b0139747820>) grad(tensor([4., 7., 9.])) tensor(tensor([10., 15., 25.], grad_fn=<MulBackward0>))\n",
            "z_after_backward requires_grad(True) is_leaf(False) retains_grad(True) grad_fn(<DotBackward0 object at 0x7b0139747130>) grad(1.0) tensor(tensor(370., grad_fn=<DotBackward0>))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-f748d96fc13e>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 14"
      ],
      "metadata": {
        "id": "s20KCSmRjLLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_info(tensor):\n",
        "  info = []\n",
        "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
        "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
        "  info.append(f'tensor({str(tensor)})')\n",
        "  return ' '.join(info)\n",
        "\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "y = x * torch.tensor([2.0, 3.0, 5.0])\n",
        "w = y.detach()\n",
        "z = w @ torch.tensor([4.0, 7.0, 9.0])\n",
        "\n",
        "print('x', get_tensor_info(x))\n",
        "print('y', get_tensor_info(y))\n",
        "print('w', get_tensor_info(w))\n",
        "print('z', get_tensor_info(z))\n",
        "\n",
        "z.backward()\n",
        "\n",
        "print('x_after_backward', get_tensor_info(x))\n",
        "print('y_after_backward', get_tensor_info(y))\n",
        "print('w_after_backward', get_tensor_info(w))\n",
        "print('z_after_backward', get_tensor_info(z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "lEk_EICWjBD8",
        "outputId": "57ea09ea-b13d-4193-f7c8-bc8402f9b98a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
            "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<MulBackward0 object at 0x7b0139745fc0>) grad(None) tensor(tensor([10., 15., 25.], grad_fn=<MulBackward0>))\n",
            "w requires_grad(False) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor([10., 15., 25.]))\n",
            "z requires_grad(False) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(370.))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-9f91a12e145c>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-9f91a12e145c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_tensor_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x_after_backward'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_tensor_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 15"
      ],
      "metadata": {
        "id": "xDa_oQHFjP3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_info(tensor):\n",
        "  info = []\n",
        "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
        "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
        "  info.append(f'tensor({str(tensor)})')\n",
        "  return ' '.join(info)\n",
        "\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "y = x * torch.tensor([2.0, 3.0, 5.0])\n",
        "w = y.detach()\n",
        "w.requires_grad_()\n",
        "z = w @ torch.tensor([4.0, 7.0, 9.0])\n",
        "\n",
        "print('x', get_tensor_info(x))\n",
        "print('y', get_tensor_info(y))\n",
        "print('w', get_tensor_info(w))\n",
        "print('z', get_tensor_info(z))\n",
        "\n",
        "z.backward()\n",
        "\n",
        "print('x_after_backward', get_tensor_info(x))\n",
        "print('y_after_backward', get_tensor_info(y))\n",
        "print('w_after_backward', get_tensor_info(w))\n",
        "print('z_after_backward', get_tensor_info(z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEDgyPA-jOnH",
        "outputId": "c0307a2b-bbc2-456a-ba93-1a6b3cfafa9a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
            "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<MulBackward0 object at 0x7b01397458d0>) grad(None) tensor(tensor([10., 15., 25.], grad_fn=<MulBackward0>))\n",
            "w requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor([10., 15., 25.], requires_grad=True))\n",
            "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<DotBackward0 object at 0x7b0139747c70>) grad(None) tensor(tensor(370., grad_fn=<DotBackward0>))\n",
            "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
            "y_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<MulBackward0 object at 0x7b0139747c70>) grad(None) tensor(tensor([10., 15., 25.], grad_fn=<MulBackward0>))\n",
            "w_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(tensor([4., 7., 9.])) tensor(tensor([10., 15., 25.], requires_grad=True))\n",
            "z_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<DotBackward0 object at 0x7b01397460e0>) grad(None) tensor(tensor(370., grad_fn=<DotBackward0>))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-2fa14b2213be>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 16"
      ],
      "metadata": {
        "id": "CUC4TumjjUiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_info(tensor):\n",
        "  info = []\n",
        "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
        "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
        "  info.append(f'tensor({str(tensor)})')\n",
        "  return ' '.join(info)\n",
        "\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "y = x * torch.tensor([2.0, 3.0, 5.0])\n",
        "w = y.detach()\n",
        "w.requires_grad_()\n",
        "z = w @ torch.tensor([4.0, 7.0, 9.0])\n",
        "\n",
        "print('x', get_tensor_info(x))\n",
        "print('y', get_tensor_info(y))\n",
        "print('w', get_tensor_info(w))\n",
        "print('z', get_tensor_info(z))\n",
        "\n",
        "z.backward()\n",
        "y.backward(gradient=w.grad)\n",
        "\n",
        "print('x_after_backward', get_tensor_info(x))\n",
        "print('y_after_backward', get_tensor_info(y))\n",
        "print('w_after_backward', get_tensor_info(w))\n",
        "print('z_after_backward', get_tensor_info(z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biKhQoYHjTPu",
        "outputId": "7a16b83d-db95-4ab5-9ed5-ad9e8b3c0eb8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
            "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<MulBackward0 object at 0x7b0139744640>) grad(None) tensor(tensor([10., 15., 25.], grad_fn=<MulBackward0>))\n",
            "w requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor([10., 15., 25.], requires_grad=True))\n",
            "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<DotBackward0 object at 0x7b0139744640>) grad(None) tensor(tensor(370., grad_fn=<DotBackward0>))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-6c58b45eff93>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(74.0) tensor(tensor(5., requires_grad=True))\n",
            "y_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<MulBackward0 object at 0x7b01387ebbb0>) grad(None) tensor(tensor([10., 15., 25.], grad_fn=<MulBackward0>))\n",
            "w_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(tensor([4., 7., 9.])) tensor(tensor([10., 15., 25.], requires_grad=True))\n",
            "z_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<DotBackward0 object at 0x7b01387ebe80>) grad(None) tensor(tensor(370., grad_fn=<DotBackward0>))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-6c58b45eff93>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 17"
      ],
      "metadata": {
        "id": "Nj8agA7cjZCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_info(tensor):\n",
        "  info = []\n",
        "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
        "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
        "  info.append(f'tensor({str(tensor)})')\n",
        "  return ' '.join(info)\n",
        "\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "y = x ** 3\n",
        "z = torch.log(y)\n",
        "\n",
        "print('x', get_tensor_info(x))\n",
        "print('y', get_tensor_info(y))\n",
        "print('z', get_tensor_info(z))\n",
        "\n",
        "z.backward()\n",
        "\n",
        "print('x_after_backward', get_tensor_info(x))\n",
        "print('y_after_backward', get_tensor_info(y))\n",
        "print('z_after_backward', get_tensor_info(z))\n",
        "\n",
        "print('x.grad_after_backward', get_tensor_info(x.grad))\n",
        "print('y.grad_after_backward', get_tensor_info(y.grad))\n",
        "print('z.grad_after_backward', get_tensor_info(z.grad))\n",
        "\n",
        "x_2nd_grad = torch.autograd.grad(x.grad, x)\n",
        "\n",
        "print('x_2nd_grad', x_2nd_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "0ZvZZBoWjXmC",
        "outputId": "03528590-c812-4587-eae6-0ab73e4813dc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
            "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b01387ea1d0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b01387e9de0>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
            "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
            "y_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b01387e9de0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "z_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b01387ea1d0>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
            "x.grad_after_backward requires_grad(False) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(0.6000))\n",
            "y.grad_after_backward requires_grad(None) is_leaf(None) retains_grad(None) grad_fn(None) grad(None) tensor(None)\n",
            "z.grad_after_backward requires_grad(None) is_leaf(None) retains_grad(None) grad_fn(None) grad(None) tensor(None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-54228050315a>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  info.append(f'{name}({getattr(tensor, name, None)})')\n",
            "<ipython-input-30-54228050315a>:23: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print('y.grad_after_backward', get_tensor_info(y.grad))\n",
            "<ipython-input-30-54228050315a>:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print('z.grad_after_backward', get_tensor_info(z.grad))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-54228050315a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'z.grad_after_backward'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_tensor_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mx_2nd_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x_2nd_grad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_2nd_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n\u001b[1;32m    495\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         result = _engine_run_backward(\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 18"
      ],
      "metadata": {
        "id": "C-QElZbUjee-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_info(tensor):\n",
        "  info = []\n",
        "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
        "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
        "  info.append(f'tensor({str(tensor)})')\n",
        "  return ' '.join(info)\n",
        "\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "y = x ** 3\n",
        "z = torch.log(y)\n",
        "\n",
        "print('x', get_tensor_info(x))\n",
        "print('y', get_tensor_info(y))\n",
        "print('z', get_tensor_info(z))\n",
        "\n",
        "z.backward(create_graph=True)\n",
        "\n",
        "print('x_after_backward', get_tensor_info(x))\n",
        "print('y_after_backward', get_tensor_info(y))\n",
        "print('z_after_backward', get_tensor_info(z))\n",
        "\n",
        "print('x.grad_after_backward', get_tensor_info(x.grad))\n",
        "print('y.grad_after_backward', get_tensor_info(y.grad))\n",
        "print('z.grad_after_backward', get_tensor_info(z.grad))\n",
        "\n",
        "x_2nd_grad = torch.autograd.grad(x.grad, x)\n",
        "\n",
        "print('x_2nd_grad', x_2nd_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk2X2KLUjct6",
        "outputId": "f93be8eb-47e8-4332-e9de-998cf2819740"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
            "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b01397460b0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b01397446d0>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
            "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
            "y_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b01397446d0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "z_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b0139745a50>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
            "x.grad_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<CopyBackwards object at 0x7b01397446d0>) grad(None) tensor(tensor(0.6000, grad_fn=<CopyBackwards>))\n",
            "y.grad_after_backward requires_grad(None) is_leaf(None) retains_grad(None) grad_fn(None) grad(None) tensor(None)\n",
            "z.grad_after_backward requires_grad(None) is_leaf(None) retains_grad(None) grad_fn(None) grad(None) tensor(None)\n",
            "x_2nd_grad (tensor(-0.1200),)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-43a91b3b4d46>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  info.append(f'{name}({getattr(tensor, name, None)})')\n",
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:823: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /pytorch/torch/csrc/autograd/engine.cpp:1260.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "<ipython-input-31-43a91b3b4d46>:23: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print('y.grad_after_backward', get_tensor_info(y.grad))\n",
            "<ipython-input-31-43a91b3b4d46>:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print('z.grad_after_backward', get_tensor_info(z.grad))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 19"
      ],
      "metadata": {
        "id": "wuh1Vy0gjjmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_info(tensor):\n",
        "  info = []\n",
        "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
        "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
        "  info.append(f'tensor({str(tensor)})')\n",
        "  return ' '.join(info)\n",
        "\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "y = x ** 3\n",
        "z = torch.log(y)\n",
        "\n",
        "print('x', get_tensor_info(x))\n",
        "print('y', get_tensor_info(y))\n",
        "print('z', get_tensor_info(z))\n",
        "\n",
        "def hook_func(grad):\n",
        "  print('grad', grad)\n",
        "  return grad * 2\n",
        "\n",
        "hook = y.register_hook(hook_func)\n",
        "z.backward()\n",
        "hook.remove()\n",
        "\n",
        "print('x_after_backward', get_tensor_info(x))\n",
        "print('y_after_backward', get_tensor_info(y))\n",
        "print('z_after_backward', get_tensor_info(z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfrVJ7z7jiB_",
        "outputId": "c9d8d2a0-3c65-4acc-a89a-cc5722cd9be3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
            "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b01387ebcd0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b01387e9c00>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
            "grad tensor(0.0080)\n",
            "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(1.2000000476837158) tensor(tensor(5., requires_grad=True))\n",
            "y_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b01387e9c00>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "z_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7b01387ebd60>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-7c9434321345>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 20"
      ],
      "metadata": {
        "id": "AQdaIOG_jpSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyPow(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, input_1, input_2):\n",
        "    ctx.save_for_backward(input_1, input_2)\n",
        "    result = input_1 ** input_2\n",
        "    return result\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    input_1, input_2 = ctx.saved_tensors\n",
        "    grad_input_1 = grad_output * input_2 * input_1 ** (input_2 - 1)\n",
        "    grad_input_2 = grad_output * input_1 ** input_2 * torch.log(input_1)\n",
        "    return grad_input_1, grad_input_2\n",
        "\n",
        "myPow = MyPow.apply\n",
        "\n",
        "q = torch.tensor(3.0, dtype=torch.float64, requires_grad=True)\n",
        "x = torch.tensor(5.0, dtype=torch.float64, requires_grad=True)\n",
        "\n",
        "print(torch.autograd.gradcheck(myPow, (x, q)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2Tv1xP6jmg8",
        "outputId": "5b29a3b8-53bd-46cc-c302-c21900a4ca71"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 21"
      ],
      "metadata": {
        "id": "31zl6tqFjsza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyPow(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, input_1, input_2):\n",
        "    ctx.save_for_backward(input_1, input_2)\n",
        "    result = input_1 ** input_2\n",
        "    return result\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    input_1, input_2 = ctx.saved_tensors\n",
        "    grad_input_1 = grad_output * input_2 * input_1 ** (input_2 - 1)\n",
        "    grad_input_2 = grad_output * input_1 ** input_2 * torch.log(input_1 + 1)\n",
        "    return grad_input_1, grad_input_2\n",
        "\n",
        "myPow = MyPow.apply\n",
        "\n",
        "q = torch.tensor(3.0, dtype=torch.float64, requires_grad=True)\n",
        "x = torch.tensor(5.0, dtype=torch.float64, requires_grad=True)\n",
        "\n",
        "print(torch.autograd.gradcheck(myPow, (x, q)))  # Should return True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "pIKx08_njugZ",
        "outputId": "2ac57553-c4d2-4665-c16d-c48da6c71d12"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "GradcheckError",
          "evalue": "Jacobian mismatch for output 0 with respect to input 1,\nnumerical:tensor([[201.1797]], dtype=torch.float64)\nanalytical:tensor([[223.9699]], dtype=torch.float64)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mGradcheckError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-c6bd2fe49157>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyPow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Should return True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36mgradcheck\u001b[0;34m(func, inputs, eps, atol, rtol, raise_exception, nondet_tol, check_undefined_grad, check_grad_dtypes, check_batched_grad, check_batched_forward_grad, check_forward_ad, check_backward_ad, fast_mode, masked)\u001b[0m\n\u001b[1;32m   2053\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2054\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2055\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_gradcheck_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36m_gradcheck_helper\u001b[0;34m(func, inputs, eps, atol, rtol, nondet_tol, check_undefined_grad, check_grad_dtypes, check_batched_grad, check_batched_forward_grad, check_forward_ad, check_backward_ad, fast_mode, masked)\u001b[0m\n\u001b[1;32m   2082\u001b[0m         \u001b[0m_fast_gradcheck\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfast_mode\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_slow_gradcheck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m     )\n\u001b[0;32m-> 2084\u001b[0;31m     _gradcheck_real_imag(\n\u001b[0m\u001b[1;32m   2085\u001b[0m         \u001b[0mgradcheck_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36m_gradcheck_real_imag\u001b[0;34m(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps, rtol, atol, check_grad_dtypes, check_forward_ad, check_backward_ad, nondet_tol, check_undefined_grad)\u001b[0m\n\u001b[1;32m   1492\u001b[0m             )\n\u001b[1;32m   1493\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m             gradcheck_fn(\n\u001b[0m\u001b[1;32m   1495\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m                 \u001b[0mfunc_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36m_slow_gradcheck\u001b[0;34m(func, func_out, tupled_inputs, outputs, eps, rtol, atol, check_grad_dtypes, nondet_tol, use_forward_ad, complex_indices, test_imag, masked)\u001b[0m\n\u001b[1;32m   1633\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalytical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumerical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_allclose_with_type_promotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1635\u001b[0;31m                     raise GradcheckError(\n\u001b[0m\u001b[1;32m   1636\u001b[0m                         \u001b[0m_get_notallclose_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplex_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_imag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m                     )\n",
            "\u001b[0;31mGradcheckError\u001b[0m: Jacobian mismatch for output 0 with respect to input 1,\nnumerical:tensor([[201.1797]], dtype=torch.float64)\nanalytical:tensor([[223.9699]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code 22"
      ],
      "metadata": {
        "id": "4KJYXx8kkK-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tensor_info(tensor):\n",
        "  info = []\n",
        "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
        "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
        "  info.append(f'tensor({str(tensor)})')\n",
        "  return ' '.join(info)\n",
        "\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "y = x ** 3\n",
        "w = x ** 2\n",
        "z = torch.log(y) + torch.sqrt(w)\n",
        "\n",
        "print('x', get_tensor_info(x))\n",
        "print('y', get_tensor_info(y))\n",
        "print('w', get_tensor_info(w))\n",
        "print('z', get_tensor_info(z))\n",
        "\n",
        "print('z.grad_fn', z.grad_fn)\n",
        "print('z.grad_fn.next_functions', z.grad_fn.next_functions)\n",
        "print('y.grad_fn', z.grad_fn.next_functions[0][0].next_functions)\n",
        "print('x_1.grad_fn', z.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
        "print('x_1_is_x', z.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].variable is x)\n",
        "print('w.grad_fn', z.grad_fn.next_functions[1][0].next_functions)\n",
        "print('x_2.grad_fn', z.grad_fn.next_functions[1][0].next_functions[0][0].next_functions)\n",
        "print('x_2_is_x', z.grad_fn.next_functions[1][0].next_functions[0][0].next_functions[0][0].variable is x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PihpA4Ojw0L",
        "outputId": "17168417-5752-48fc-a84f-8fbb86dde1cd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
            "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b0139745840>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
            "w requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7b0139747490>) grad(None) tensor(tensor(25., grad_fn=<PowBackward0>))\n",
            "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<AddBackward0 object at 0x7b0139745840>) grad(None) tensor(tensor(9.8283, grad_fn=<AddBackward0>))\n",
            "z.grad_fn <AddBackward0 object at 0x7b0139745f00>\n",
            "z.grad_fn.next_functions ((<LogBackward0 object at 0x7b0139747490>, 0), (<SqrtBackward0 object at 0x7b0139745840>, 0))\n",
            "y.grad_fn ((<PowBackward0 object at 0x7b0139747c40>, 0),)\n",
            "x_1.grad_fn ((<AccumulateGrad object at 0x7b0139745840>, 0),)\n",
            "x_1_is_x True\n",
            "w.grad_fn ((<PowBackward0 object at 0x7b0139747490>, 0),)\n",
            "x_2.grad_fn ((<AccumulateGrad object at 0x7b0139745840>, 0),)\n",
            "x_2_is_x True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-9ba4287dd3ba>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  info.append(f'{name}({getattr(tensor, name, None)})')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Hk81OPRkN8u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}